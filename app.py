# app.py â€” TU Korea AI Management: Random Agent Simulation (Baseline)
# ì‘ì„±ì: Prof. Songhee Kang
# Update: Random Agent (No Learning)

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from scipy.stats import pearsonr, entropy
from dataclasses import dataclass
from typing import Dict, List

# ==================== 1. ê¸°ë³¸ ì„¤ì • ====================
st.set_page_config(
    page_title="í•œêµ­ê³µí•™ëŒ€: ë¬´ì‘ìœ„(Random) AI ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜", 
    page_icon="ğŸ²", 
    layout="wide"
)

# ==================== 2. ë°ì´í„° ëª¨ë¸ (í™˜ê²½) ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    rewards: Dict[str, Dict[str, float]]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

# ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
DEFAULT_SCENARIOS = [
    Scenario(
        sid="S1", title="1ë‹¨ê³„: ê³ ì „ì  íŠ¸ë¡¤ë¦¬",
        setup="ì„ ë¡œ ìœ„ 5ëª… vs 1ëª…. ë ˆë²„ë¥¼ ë‹¹ê¸¸ ê²ƒì¸ê°€?",
        options={"A": "1ëª… í¬ìƒ (ê°œì…)", "B": "ë°©ê´€ (í˜„ìƒ ìœ ì§€)"},
        rewards={"A": {"emotion": 1.0, "social": -0.5, "moral": -1.0, "identity": 0.5},
                 "B": {"emotion": -1.0, "social": 0.5, "moral": 1.0, "identity": -0.5}}
    ),
    Scenario(
        sid="S2", title="2ë‹¨ê³„: ë§¥ë½ì  ìš”ì†Œ",
        setup="ë¬´ë‹¨ ì¹¨ì…ì 5ëª… vs ê´€ë¦¬ì ìë…€ 1ëª….",
        options={"A": "5ëª… êµ¬ì¡° (ìë…€ í¬ìƒ)", "B": "ê·œì • ì¤€ìˆ˜ (5ëª… ë°©ê´€)"},
        rewards={"A": {"emotion": 0.6, "social": -0.8, "moral": -0.7, "identity": 0.3},
                 "B": {"emotion": -0.5, "social": 0.9, "moral": 0.6, "identity": 0.4}}
    ),
    Scenario(
        sid="S3", title="3ë‹¨ê³„: ì˜ë£Œ ì¬ë‚œ ë¶„ë¥˜",
        setup="ì¼ë°˜ ë¶€ìƒì ë‹¤ìˆ˜ vs ìˆ™ë ¨ëœ ì˜ì‚¬ 1ëª….",
        options={"A": "ì˜ì‚¬ ìš°ì„  (ê³µë¦¬ì£¼ì˜)", "B": "ë™ë“± ëŒ€ìš° (í‰ë“±ì£¼ì˜)"},
        rewards={"A": {"emotion": 0.7, "social": -0.4, "moral": -0.6, "identity": 0.8},
                 "B": {"emotion": -0.3, "social": 0.7, "moral": 0.9, "identity": 0.5}}
    ),
]

# ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹
CULTURES_PRESETS = {
    "USA":      {"emotion": 0.3, "social": 0.1, "identity": 0.3, "moral": 0.3},
    "CHINA":    {"emotion": 0.1, "social": 0.5, "identity": 0.2, "moral": 0.2},
    "EUROPE":   {"emotion": 0.3, "social": 0.2, "identity": 0.2, "moral": 0.3},
    "KOREA":    {"emotion": 0.2, "social": 0.2, "identity": 0.4, "moral": 0.2},
    "LATIN_AM": {"emotion": 0.4, "social": 0.2, "identity": 0.2, "moral": 0.2},
    "MIDDLE_E": {"emotion": 0.1, "social": 0.2, "identity": 0.2, "moral": 0.5},
    "AFRICA":   {"emotion": 0.2, "social": 0.4, "identity": 0.2, "moral": 0.2},
}

# ==================== 3. ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ (Random Agent) ====================
class RandomAgent:
    """
    í•™ìŠµí•˜ì§€ ì•Šê³  ë¬´ì‘ìœ„ë¡œ í–‰ë™í•˜ëŠ” ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
    E-Greedy Agentì™€ ë¹„êµí•˜ì—¬ 'í•™ìŠµì˜ íš¨ê³¼'ë¥¼ ì¦ëª…í•˜ëŠ” ëŒ€ì¡°êµ°ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    def __init__(self, name, weights, scenarios):
        self.name = name
        self.weights = weights
        self.scenarios = scenarios
        # í•™ìŠµì„ ìœ„í•œ Q-Tableì´ë‚˜ Learning Rateê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
        
    def get_action(self, sid):
        """
        [í–‰ë™ ì„ íƒ]
        ê°€ì¹˜ íŒë‹¨ ì—†ì´ ë™ì „ì„ ë˜ì§€ë“¯ 50:50 í™•ë¥ ë¡œ ì„ íƒí•©ë‹ˆë‹¤.
        """
        return random.choice(["A", "B"])

    def calculate_reward(self, sid, action):
        """
        [ë³´ìƒ ê³„ì‚°]
        í–‰ë™ì€ ëœë¤ì´ì§€ë§Œ, ê·¸ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ìœ¤ë¦¬ì ì¸ì§€(ì ìˆ˜)ëŠ” ê³„ì‚°í•©ë‹ˆë‹¤.
        ì´ë¥¼ í†µí•´ 'ëœë¤ ì „ëµì˜ ì„±ê³¼'ë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        scn = next(s for s in self.scenarios if s.sid == sid)
        r_vec = scn.rewards[action]
        reward = sum(r_vec.get(k, 0) * self.weights.get(k, 0) for k in FRAMEWORKS) * 10
        return reward

    def update(self, sid, action, reward):
        """
        [í•™ìŠµ ë¶ˆê°€]
        Random AgentëŠ” ê²½í—˜ì„ í†µí•´ ë°°ìš°ì§€ ì•Šìœ¼ë¯€ë¡œ, ì•„ë¬´ê²ƒë„ ì—…ë°ì´íŠ¸í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        """
        pass

    def get_avg_entropy(self):
        """
        [ì „ëµ ì—”íŠ¸ë¡œí”¼]
        í•­ìƒ 50:50 í™•ë¥ ë¡œ ì°ìœ¼ë¯€ë¡œ, ë¶ˆí™•ì‹¤ì„±(ì—”íŠ¸ë¡œí”¼)ì€ í•­ìƒ ìµœëŒ€ê°’ì…ë‹ˆë‹¤.
        p=[0.5, 0.5]ì¼ ë•Œ entropy â‰ˆ 0.693
        """
        return entropy([0.5, 0.5])

# ==================== 4. ë¶„ì„ ë„êµ¬ ====================
def calculate_diversity(actions_list: List[str]) -> float:
    if not actions_list: return 0.0
    a_count = actions_list.count("A")
    ratio = a_count / len(actions_list)
    return 1.0 - (2 * abs(0.5 - ratio))

def run_simulation(culture_name, weights, episodes, custom_scenarios):
    # E-Greedy ëŒ€ì‹  RandomAgent ì‚¬ìš©
    agent = RandomAgent(culture_name, weights, custom_scenarios)
    
    history = {
        "episode": [],
        "reward": [],
        "diversity": [],
        "entropy": []
    }
    
    progress = st.progress(0)
    
    for ep in range(episodes):
        ep_actions = []
        ep_reward = 0
        
        for scn in custom_scenarios:
            # 1. ë¬´ì‘ìœ„ í–‰ë™ ì„ íƒ
            action = agent.get_action(scn.sid)
            
            # 2. ê²°ê³¼(ë³´ìƒ) í™•ì¸
            reward = agent.calculate_reward(scn.sid, action)
            
            # 3. í•™ìŠµí•˜ì§€ ì•ŠìŒ (Update ìƒëµ)
            agent.update(scn.sid, action, reward)
            
            ep_actions.append(action)
            ep_reward += reward
        
        history["episode"].append(ep + 1)
        history["reward"].append(ep_reward)
        history["diversity"].append(calculate_diversity(ep_actions))
        history["entropy"].append(agent.get_avg_entropy())
        
        if (ep + 1) % 10 == 0:
            progress.progress((ep + 1) / episodes)
            
    progress.empty()
    return pd.DataFrame(history)

# ==================== 5. UI êµ¬ì„± ====================
st.title("ğŸ² í•œêµ­ê³µí•™ëŒ€: ë¬´ì‘ìœ„(Random) AI ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜")
st.markdown("""
ì´ ì‹œë®¬ë ˆì´í„°ëŠ” **í•™ìŠµ ëŠ¥ë ¥ì´ ì—†ëŠ” ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸**(Random Agent)ë¥¼ êµ¬ë™í•©ë‹ˆë‹¤.
E-Greedy í•™ìŠµ ëª¨ë¸ê³¼ ë¹„êµí•˜ì—¬ **"ì™œ í•™ìŠµì´ ì¤‘ìš”í•œê°€?"** ë¥¼ ë³´ì—¬ì£¼ëŠ” ë¹„êµ ì‹¤í—˜ìš©ì…ë‹ˆë‹¤.

- **íŠ¹ì§•**: ëª¨ë“  ì„ íƒì„ ë™ì „ ë˜ì§€ê¸°(50:50)ë¡œ ê²°ì •í•©ë‹ˆë‹¤.
- **ì˜ˆìƒ ê²°ê³¼**: ë³´ìƒì´ ì˜¤ë¥´ì§€ ì•Šê³  ì œìë¦¬ ê±¸ìŒì„ í•˜ë©°, ì „ëµì˜ ë³€í™”(ì—”íŠ¸ë¡œí”¼ ê°ì†Œ)ê°€ ì—†ìŠµë‹ˆë‹¤.
""")

# --- [ì‚¬ì´ë“œë°”] ì—ì´ì „íŠ¸ ì„¤ì • ---
st.sidebar.header("ğŸ‘¤ 2. ì—ì´ì „íŠ¸(ë¬¸í™”ê¶Œ) ì„¤ì •")
selected_culture = st.sidebar.selectbox("ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹", list(CULTURES_PRESETS.keys()), index=3)
episodes = st.sidebar.slider("ì‹œë®¬ë ˆì´ì…˜ íšŸìˆ˜", 100, 1000, 300, step=50)

st.sidebar.subheader("ê°€ì¹˜ê´€ ê°€ì¤‘ì¹˜ ì¡°ì •")
mod_weights = {}
culture_defaults = CULTURES_PRESETS[selected_culture]
for k in FRAMEWORKS:
    mod_weights[k] = st.sidebar.slider(f"{k.capitalize()}", 0.0, 1.0, culture_defaults[k])
total_w = sum(mod_weights.values()) or 1
final_weights = {k: v/total_w for k, v in mod_weights.items()}

st.sidebar.markdown("---")
st.sidebar.json(final_weights)

# --- [ë©”ì¸] í™˜ê²½ ì„¤ì • ---
st.header("ğŸŒ 1. í™˜ê²½(ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒ) ì„¤ì •")
st.info("ì‹œë‚˜ë¦¬ì˜¤ì˜ ë³´ìƒ ì ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ, Random AgentëŠ” ì´ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ë§‰ë¬´ê°€ë‚´ë¡œ ì„ íƒí•©ë‹ˆë‹¤.")

custom_scenarios = []
tabs = st.tabs([s.title for s in DEFAULT_SCENARIOS])

for i, (tab, default_scn) in enumerate(zip(tabs, DEFAULT_SCENARIOS)):
    with tab:
        st.markdown(f"> **ìƒí™©:** {default_scn.setup}")
        col_a, col_b = st.columns(2)
        with col_a:
            st.markdown(f"### ğŸ…° {default_scn.options['A']}")
            r_a = default_scn.rewards["A"].copy()
            for fw in FRAMEWORKS:
                r_a[fw] = st.slider(f"[A] {fw}", -1.0, 1.0, r_a.get(fw,0.0), 0.1, key=f"s{i}a_{fw}")
        with col_b:
            st.markdown(f"### ğŸ…± {default_scn.options['B']}")
            r_b = default_scn.rewards["B"].copy()
            for fw in FRAMEWORKS:
                r_b[fw] = st.slider(f"[B] {fw}", -1.0, 1.0, r_b.get(fw,0.0), 0.1, key=f"s{i}b_{fw}")
        custom_scenarios.append(Scenario(default_scn.sid, default_scn.title, default_scn.setup, default_scn.options, {"A": r_a, "B": r_b}))

# --- [ë¶„ì„ ì‹¤í–‰] ---
st.divider()
st.header("ğŸš€ 3. ì‹œë®¬ë ˆì´ì…˜ ë° ë¶„ì„")

if st.button("ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ (Random)", type="primary"):
    with st.spinner("AIê°€ ë¬´ì‘ìœ„ë¡œ ì„ íƒì„ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤..."):
        df = run_simulation(selected_culture, final_weights, episodes, custom_scenarios)
    
    st.warning("âš ï¸ **ì£¼ì˜**: ì´ê²ƒì€ í•™ìŠµí•˜ì§€ ì•ŠëŠ” ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ì…ë‹ˆë‹¤.")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.subheader("ğŸ“‰ ì´ ë³´ìƒ (Reward)")
        st.caption("ìš°ìƒí–¥í•˜ì§€ ì•Šê³  ë¶ˆê·œì¹™í•˜ê²Œ ì§„ë™í•©ë‹ˆë‹¤.")
        st.line_chart(df, x="episode", y="reward", color="#FF4B4B")
        
    with col2:
        st.subheader("â– ì „ëµ ì—”íŠ¸ë¡œí”¼ (Entropy)")
        st.caption("ë–¨ì–´ì§€ì§€ ì•Šê³  ë†’ì€ ë¶ˆí™•ì‹¤ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
        st.line_chart(df, x="episode", y="entropy", color="#2CA02C")
        
    with col3:
        st.subheader("ğŸ”€ í–‰ë™ ë‹¤ì–‘ì„± (Diversity)")
        st.caption("í•­ìƒ 1.0(ìµœëŒ€ ë‹¤ì–‘ì„±) ê·¼ì²˜ì— ë¨¸ë­…ë‹ˆë‹¤.")
        st.line_chart(df, x="episode", y="diversity", color="#1F77B4")
        
    # ìƒê´€ê´€ê³„ ë¶„ì„
    st.markdown("---")
    st.subheader("ğŸ”— ë‹¤ì–‘ì„±ê³¼ ë³´ìƒì˜ ìƒê´€ê´€ê³„")
    
    r_val, p_val = pearsonr(df["diversity"], df["reward"])
    
    c_plot, c_stat = st.columns([2, 1])
    with c_plot:
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.scatter(df["diversity"], df["reward"], alpha=0.6, c='gray', edgecolors='w')
        if len(df) > 1:
            z = np.polyfit(df["diversity"], df["reward"], 1)
            p = np.poly1d(z)
            ax.plot(df["diversity"], p(df["diversity"]), "r--", label="Trend")
        ax.set_xlabel("Diversity (0=í¸í–¥, 1=ê· í˜•)")
        ax.set_ylabel("Total Reward")
        ax.set_title(f"Random Walk Scatter (r={r_val:.2f})")
        ax.grid(True, alpha=0.3); ax.legend()
        st.pyplot(fig)
        
    with c_stat:
        st.metric("í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (r)", f"{r_val:.3f}")
        st.write("ë¬´ì‘ìœ„ ì—ì´ì „íŠ¸ì—ì„œëŠ” ì˜ë¯¸ ìˆëŠ” ìƒê´€ê´€ê³„ê°€ ë‚˜íƒ€ë‚˜ì§€ ì•Šê±°ë‚˜, ìš°ì—°ì— ì˜í•œ ê²°ê³¼ì¼ ë¿ì…ë‹ˆë‹¤.")

    # ë‹¤ìš´ë¡œë“œ
    with st.expander("ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ"):
        st.dataframe(df.head())
        st.download_button("CSVë¡œ ì €ì¥", df.to_csv(index=False), "random_agent_data.csv")
