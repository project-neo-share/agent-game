# app.py â€” Cultural AI Ethics: Single Culture & Scenario Config
# ì‘ì„±ì: Prof. Songhee Kang
# Update: Restored Scenario Reward Config + Diversity-Reward Correlation

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from scipy.stats import pearsonr
from dataclasses import dataclass
from typing import Dict, List

# ==================== ì„¤ì • ====================
st.set_page_config(page_title="AI Ethics: Environment & Agent", page_icon="ğŸ›ï¸", layout="wide")

# ==================== ë°ì´í„° ëª¨ë¸ ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    rewards: Dict[str, Dict[str, float]]

# ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° (Default Presets)
DEFAULT_SCENARIOS = [
    Scenario(
        sid="S1", title="1ë‹¨ê³„: ê³ ì „ì  íŠ¸ë¡¤ë¦¬",
        setup="ì„ ë¡œ ìœ„ 5ëª… vs 1ëª…. ë ˆë²„ë¥¼ ë‹¹ê¸¸ ê²ƒì¸ê°€?",
        options={"A": "1ëª… í¬ìƒ (ê°œì…)", "B": "ë°©ê´€ (í˜„ìƒ ìœ ì§€)"},
        rewards={"A": {"emotion": 1.0, "social": -0.5, "moral": -1.0, "identity": 0.5},
                 "B": {"emotion": -1.0, "social": 0.5, "moral": 1.0, "identity": -0.5}}
    ),
    Scenario(
        sid="S2", title="2ë‹¨ê³„: ë§¥ë½ì  ìš”ì†Œ",
        setup="ë¬´ë‹¨ ì¹¨ì…ì 5ëª… vs ê´€ë¦¬ì ìë…€ 1ëª….",
        options={"A": "5ëª… êµ¬ì¡° (ìë…€ í¬ìƒ)", "B": "ê·œì • ì¤€ìˆ˜ (5ëª… ë°©ê´€)"},
        rewards={"A": {"emotion": 0.6, "social": -0.8, "moral": -0.7, "identity": 0.3},
                 "B": {"emotion": -0.5, "social": 0.9, "moral": 0.6, "identity": 0.4}}
    ),
    Scenario(
        sid="S3", title="3ë‹¨ê³„: ì˜ë£Œ ì¬ë‚œ ë¶„ë¥˜",
        setup="ì¼ë°˜ ë¶€ìƒì ë‹¤ìˆ˜ vs ìˆ™ë ¨ëœ ì˜ì‚¬ 1ëª….",
        options={"A": "ì˜ì‚¬ ìš°ì„  (ê³µë¦¬ì£¼ì˜)", "B": "ë™ë“± ëŒ€ìš° (í‰ë“±ì£¼ì˜)"},
        rewards={"A": {"emotion": 0.7, "social": -0.4, "moral": -0.6, "identity": 0.8},
                 "B": {"emotion": -0.3, "social": 0.7, "moral": 0.9, "identity": 0.5}}
    ),
]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

# ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹
CULTURES_PRESETS = {
    "USA":      {"emotion": 0.3, "social": 0.1, "identity": 0.3, "moral": 0.3},
    "CHINA":    {"emotion": 0.1, "social": 0.5, "identity": 0.2, "moral": 0.2},
    "EUROPE":   {"emotion": 0.3, "social": 0.2, "identity": 0.2, "moral": 0.3},
    "KOREA":    {"emotion": 0.2, "social": 0.2, "identity": 0.4, "moral": 0.2},
    "LATIN_AM": {"emotion": 0.4, "social": 0.2, "identity": 0.2, "moral": 0.2},
    "MIDDLE_E": {"emotion": 0.1, "social": 0.2, "identity": 0.2, "moral": 0.5},
    "AFRICA":   {"emotion": 0.2, "social": 0.4, "identity": 0.2, "moral": 0.2},
}

# ==================== ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ====================
class QLearningAgent:
    def __init__(self, name, weights, scenarios, learning_rate=0.1, epsilon=0.5):
        self.name = name
        self.weights = weights      # ë¬¸í™”ê¶Œ ê°€ì¤‘ì¹˜ (Agent Internal)
        self.scenarios = scenarios  # ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒ í™˜ê²½ (Environment External)
        self.lr = learning_rate
        self.epsilon = epsilon
        self.q_table = {s.sid: {"A": 0.0, "B": 0.0} for s in scenarios}
        
    def get_action(self, sid):
        # Epsilon-Greedy Strategy
        if random.random() < self.epsilon:
            return random.choice(["A", "B"])
        qs = self.q_table[sid]
        if qs["A"] > qs["B"]: return "A"
        elif qs["B"] > qs["A"]: return "B"
        return random.choice(["A", "B"])

    def calculate_reward(self, sid, action):
        # í•µì‹¬ ë¡œì§: ì‹œë‚˜ë¦¬ì˜¤ê°€ ì£¼ëŠ” ë³´ìƒ ë²¡í„°(Env)ì™€ ë‚´ ê°€ì¹˜ê´€(Agent)ì˜ ë‚´ì 
        scn = next(s for s in self.scenarios if s.sid == sid)
        r_vec = scn.rewards[action]
        reward = sum(r_vec.get(k, 0) * self.weights.get(k, 0) for k in FRAMEWORKS) * 10
        return reward

    def update(self, sid, action, reward):
        old_q = self.q_table[sid][action]
        self.q_table[sid][action] = old_q + self.lr * (reward - old_q)

    def decay_epsilon(self):
        self.epsilon = max(0.01, self.epsilon * 0.99)

# ==================== ë¶„ì„ í•¨ìˆ˜ ====================
def calculate_diversity(actions_list: List[str]) -> float:
    """í–‰ë™ ë‹¤ì–‘ì„± ê³„ì‚° (1.0 = A/B ê· í˜•, 0.0 = í•œìª½ ì ë¦¼)"""
    if not actions_list: return 0.0
    a_count = actions_list.count("A")
    ratio = a_count / len(actions_list)
    return 1.0 - (2 * abs(0.5 - ratio))

def run_simulation(culture_name, weights, episodes, custom_scenarios):
    agent = QLearningAgent(culture_name, weights, custom_scenarios)
    
    history = {
        "episode": [],
        "reward": [],
        "diversity": []
    }
    
    progress = st.progress(0)
    
    for ep in range(episodes):
        ep_actions = []
        ep_reward = 0
        
        for scn in custom_scenarios:
            action = agent.get_action(scn.sid)
            reward = agent.calculate_reward(scn.sid, action)
            agent.update(scn.sid, action, reward)
            
            ep_actions.append(action)
            ep_reward += reward
            
        agent.decay_epsilon()
        
        history["episode"].append(ep + 1)
        history["reward"].append(ep_reward)
        history["diversity"].append(calculate_diversity(ep_actions))
        
        if (ep + 1) % 10 == 0:
            progress.progress((ep + 1) / episodes)
            
    progress.empty()
    return pd.DataFrame(history)

# ==================== UI êµ¬ì„± ====================
st.title("ğŸ›ï¸ AI Ethics Simulation: Config & Analysis")
st.markdown("""
**1ë‹¨ê³„ (í™˜ê²½ ì„¤ì •):** ê° ì‹œë‚˜ë¦¬ì˜¤ì˜ ì„ íƒì§€ê°€ ì£¼ëŠ” ë³´ìƒ(Reward Vector)ì„ ì„¤ì •í•©ë‹ˆë‹¤.<br>
**2ë‹¨ê³„ (ì—ì´ì „íŠ¸ ì„¤ì •):** íŠ¹ì • ë¬¸í™”ê¶Œì˜ ê°€ì¹˜ê´€ ê°€ì¤‘ì¹˜(Weights)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.<br>
**3ë‹¨ê³„ (ë¶„ì„):** í–‰ë™ ë‹¤ì–‘ì„±ê³¼ ë³´ìƒ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
""", unsafe_allow_html=True)

# --- [ì‚¬ì´ë“œë°”] ì—ì´ì „íŠ¸(ë¬¸í™”ê¶Œ) ì„¤ì • ---
st.sidebar.header("ğŸ‘¤ 2. Agent (Culture) Setup")
selected_culture = st.sidebar.selectbox("ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹ ì„ íƒ", list(CULTURES_PRESETS.keys()), index=3)
episodes = st.sidebar.slider("í•™ìŠµ íšŸìˆ˜ (Episodes)", 100, 1000, 300, step=50)

st.sidebar.subheader("ê°€ì¹˜ê´€ ê°€ì¤‘ì¹˜ ë¯¸ì„¸ì¡°ì •")
st.sidebar.caption("ë¬¸í™”ê¶Œì˜ ê¸°ë³¸ ì„±í–¥ì„ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
culture_weights = CULTURES_PRESETS[selected_culture].copy()

# ì‚¬ì´ë“œë°”ì—ì„œ ê°€ì¤‘ì¹˜ ì¡°ì • UI
mod_weights = {}
for k in FRAMEWORKS:
    mod_weights[k] = st.sidebar.slider(f"{k.capitalize()}", 0.0, 1.0, culture_weights[k])

# ê°€ì¤‘ì¹˜ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡)
total_w = sum(mod_weights.values()) or 1
final_weights = {k: v/total_w for k, v in mod_weights.items()}

st.sidebar.markdown("---")
st.sidebar.write("ğŸ“Š **ì ìš©ëœ ê°€ì¤‘ì¹˜:**")
st.sidebar.json(final_weights)

# --- [ë©”ì¸ í™”ë©´] ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒ ë²¡í„° ì„¤ì • (ë³µêµ¬ëœ ê¸°ëŠ¥) ---
st.header("ğŸŒ 1. Environment (Scenario) Setup")
st.info("ê° ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ A/B ì„ íƒì§€ê°€ ì£¼ëŠ” ë³´ìƒ(ì„±ê²©)ì„ ì •ì˜í•©ë‹ˆë‹¤. (-1.0: ë¶€ì •ì  ~ 1.0: ê¸ì •ì )")

custom_scenarios = []

# ì‹œë‚˜ë¦¬ì˜¤ ë£¨í”„ (3ê°œ)
cols = st.columns(3) # ê°€ë¡œë¡œ ë°°ì¹˜
for i, default_scn in enumerate(DEFAULT_SCENARIOS):
    with cols[i]:
        with st.expander(f"ğŸ“ {default_scn.title}", expanded=True):
            st.caption(default_scn.setup)
            
            # Option A ì„¤ì •
            st.markdown(f"**ğŸ…° {default_scn.options['A']}**")
            r_a = default_scn.rewards["A"].copy()
            # ê³µê°„ ì ˆì•½ì„ ìœ„í•´ Emotionê³¼ Moralë§Œ ì˜ˆì‹œë¡œ í‘œì‹œ (í•„ìš”ì‹œ ì¶”ê°€ ê°€ëŠ¥)
            r_a["emotion"] = st.slider(f"S{i+1}-A Emotion", -1.0, 1.0, r_a["emotion"], key=f"s{i}a_em")
            r_a["moral"] = st.slider(f"S{i+1}-A Moral", -1.0, 1.0, r_a["moral"], key=f"s{i}a_mo")
            
            # Option B ì„¤ì •
            st.markdown(f"**ğŸ…± {default_scn.options['B']}**")
            r_b = default_scn.rewards["B"].copy()
            r_b["emotion"] = st.slider(f"S{i+1}-B Emotion", -1.0, 1.0, r_b["emotion"], key=f"s{i}b_em")
            r_b["moral"] = st.slider(f"S{i+1}-B Moral", -1.0, 1.0, r_b["moral"], key=f"s{i}b_mo")
            
            # ë‚˜ë¨¸ì§€ ê°’ë“¤ì€ ê¸°ë³¸ê°’ ìœ ì§€í•˜ë©´ì„œ ì»¤ìŠ¤í…€ ì‹œë‚˜ë¦¬ì˜¤ ê°ì²´ ìƒì„±
            custom_scenarios.append(Scenario(
                default_scn.sid, default_scn.title, default_scn.setup, 
                default_scn.options, {"A": r_a, "B": r_b}
            ))

# --- [ë¶„ì„ ì‹¤í–‰] ---
st.divider()
st.header("ğŸš€ 3. Simulation & Analysis")

if st.button("ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ (Run Analysis)", type="primary"):
    with st.spinner(f"'{selected_culture}' ì—ì´ì „íŠ¸ê°€ ì»¤ìŠ¤í…€ í™˜ê²½ì—ì„œ í•™ìŠµ ì¤‘..."):
        df = run_simulation(selected_culture, final_weights, episodes, custom_scenarios)
    
    st.success("ë¶„ì„ ì™„ë£Œ!")
    
    # 1. ê·¸ë˜í”„ ì˜ì—­ (í•™ìŠµ ê³¡ì„  & ë‹¤ì–‘ì„±)
    c1, c2 = st.columns(2)
    with c1:
        st.subheader("ğŸ“ˆ Reward Curve")
        st.line_chart(df, x="episode", y="reward", color="#FF4B4B")
    with c2:
        st.subheader("ğŸ”€ Diversity Curve")
        st.line_chart(df, x="episode", y="diversity", color="#1F77B4")
        
    # 2. ìƒê´€ê´€ê³„ ë¶„ì„ ì˜ì—­
    st.markdown("---")
    st.subheader("ğŸ”— Correlation: Diversity vs Reward")
    
    # í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜
    r_val, p_val = pearsonr(df["diversity"], df["reward"])
    
    col_plot, col_stat = st.columns([2, 1])
    
    with col_plot:
        # Scatter Plot
        fig, ax = plt.subplots(figsize=(8, 4))
        ax.scatter(df["diversity"], df["reward"], alpha=0.6, c='purple', edgecolors='w')
        
        # ì¶”ì„¸ì„  ì¶”ê°€
        z = np.polyfit(df["diversity"], df["reward"], 1)
        p = np.poly1d(z)
        ax.plot(df["diversity"], p(df["diversity"]), "r--", alpha=0.8, label="Trend")
        
        ax.set_xlabel("Behavioral Diversity (0=Rigid, 1=Flexible)")
        ax.set_ylabel("Total Reward")
        ax.set_title(f"Diversity vs Reward (r={r_val:.2f})")
        ax.legend()
        ax.grid(True, alpha=0.3)
        st.pyplot(fig)
        
    with col_stat:
        st.markdown("### ğŸ“Š í†µê³„ ìš”ì•½")
        st.metric("ìƒê´€ê³„ìˆ˜ (Pearson r)", f"{r_val:.3f}")
        st.metric("P-value", f"{p_val:.3e}")
        
        st.markdown("#### í•´ì„")
        if r_val > 0.3:
            st.success("âœ… **ì–‘ì˜ ìƒê´€ê´€ê³„**\n\në‹¤ì–‘í•œ ì „ëµì„ ì‹œë„í• ìˆ˜ë¡ ë³´ìƒì´ ë†’ì•„ì§€ëŠ” í™˜ê²½ì…ë‹ˆë‹¤.")
        elif r_val < -0.3:
            st.warning("âš ï¸ **ìŒì˜ ìƒê´€ê´€ê³„**\n\níŠ¹ì • í–‰ë™(ê·œì¹™)ì„ ê³ ìˆ˜í•´ì•¼ ë³´ìƒì´ ë†’ì€ í™˜ê²½ì…ë‹ˆë‹¤.")
        else:
            st.info("âº **ìƒê´€ì—†ìŒ**\n\në‹¤ì–‘ì„±ê³¼ ë³´ìƒ ê°„ì— ëšœë ·í•œ ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    with st.expander("ğŸ“¥ ë¡œìš° ë°ì´í„° ë‹¤ìš´ë¡œë“œ"):
        st.dataframe(df.head())
        st.download_button("CSV Save", df.to_csv(index=False), "ethics_sim_data.csv")
