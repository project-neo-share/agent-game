# app.py â€” Ethical Crossroads RL (Customizable Scenario Rewards)
# ìž‘ì„±ìž: Prof. Songhee Kang
# Update: Culture Input & Per-Scenario Reward Vector Configuration

import os, json, math, datetime as dt
import random
import numpy as np
import pandas as pd
import streamlit as st
from dataclasses import dataclass, field
from typing import Dict, List

# ==================== ì„¤ì • ====================
st.set_page_config(page_title="Ethical Crossroads RL (Custom)", page_icon="ðŸ§­", layout="wide")

# ==================== ë°ì´í„° ëª¨ë¸ ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    votes: Dict[str, str]
    base: Dict[str, Dict[str, float]]
    rewards: Dict[str, Dict[str, float]] # ì´ ë¶€ë¶„ì´ ì‚¬ìš©ìž ìž…ë ¥ì— ë”°ë¼ ë³€ê²½ë¨

# ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° (Default Presets)
DEFAULT_SCENARIOS = [
    Scenario(
        sid="S1", title="1ë‹¨ê³„: ê³ ì „ì  íŠ¸ë¡¤ë¦¬",
        setup="ì„ ë¡œ ìœ„ 5ëª… vs 1ëª…. ë ˆë²„ë¥¼ ë‹¹ê¸¸ ê²ƒì¸ê°€?",
        options={"A": "1ëª… í¬ìƒ, 5ëª… êµ¬ì¡°", "B": "ë°©ê´€ (í˜„ìƒ ìœ ì§€)"},
        votes={"emotion":"A","social":"B","moral":"B","identity":"A"},
        base={"A": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.35, "rule_violation":0.60},
              "B": {"lives_saved":0, "lives_harmed":5, "fairness_gap":0.50, "rule_violation":0.20}},
        rewards={"A": {"emotion": 1.0, "social": -0.5, "moral": -1.0, "identity": 0.5},
                 "B": {"emotion": -1.0, "social": 0.5, "moral": 1.0, "identity": -0.5}}
    ),
    Scenario(
        sid="S2", title="2ë‹¨ê³„: ë§¥ë½ì  ìš”ì†Œ",
        setup="ë¬´ë‹¨ ì¹¨ìž…ìž 5ëª… vs ê´€ë¦¬ìž ìžë…€ 1ëª….",
        options={"A": "5ëª… êµ¬ì¡° (ìžë…€ í¬ìƒ)", "B": "ê·œì • ì¤€ìˆ˜ (5ëª… ë°©ê´€)"},
        votes={"emotion":"A","social":"B","moral":"B","identity":"B"},
        base={"A": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.65, "rule_violation":0.60},
              "B": {"lives_saved":0, "lives_harmed":5, "fairness_gap":0.45, "rule_violation":0.25}},
        rewards={"A": {"emotion": 0.6, "social": -0.8, "moral": -0.7, "identity": 0.3},
                 "B": {"emotion": -0.5, "social": 0.9, "moral": 0.6, "identity": 0.4}}
    ),
    Scenario(
        sid="S3", title="3ë‹¨ê³„: ì˜ë£Œ ìž¬ë‚œ ë¶„ë¥˜",
        setup="ì¼ë°˜ ë¶€ìƒìž vs ìˆ™ë ¨ëœ ì˜ì‚¬(ìž ìž¬ ê°€ì¹˜).",
        options={"A": "ì˜ì‚¬ ìš°ì„  (ê³µë¦¬ì£¼ì˜)", "B": "ë™ë“± ëŒ€ìš° (í‰ë“±ì£¼ì˜)"},
        votes={"emotion":"A","social":"B","moral":"B","identity":"A"},
        base={"A": {"lives_saved":7, "lives_harmed":3, "fairness_gap":0.45, "rule_violation":0.35},
              "B": {"lives_saved":6, "lives_harmed":4, "fairness_gap":0.30, "rule_violation":0.10}},
        rewards={"A": {"emotion": 0.7, "social": -0.4, "moral": -0.6, "identity": 0.8},
                 "B": {"emotion": -0.3, "social": 0.7, "moral": 0.9, "identity": 0.5}}
    ),
]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

# ==================== ê°•í™”í•™ìŠµ (RL) ì—ì´ì „íŠ¸ ====================
class QLearningAgent:
    def __init__(self, scenarios, learning_rate=0.1, epsilon=0.1):
        # ì‹œë‚˜ë¦¬ì˜¤ IDê°€ ë™ì ìœ¼ë¡œ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤ê³  ê°€ì •
        self.q_table = {s.sid: {"A": 0.0, "B": 0.0} for s in scenarios}
        self.lr = learning_rate
        self.epsilon = epsilon
        
    def get_action(self, state_id: str, explore: bool = True) -> str:
        if explore and random.random() < self.epsilon:
            return random.choice(["A", "B"])
        qs = self.q_table[state_id]
        if qs["A"] > qs["B"]: return "A"
        elif qs["B"] > qs["A"]: return "B"
        return random.choice(["A", "B"])

    def update(self, state_id: str, action: str, reward: float):
        old_q = self.q_table[state_id][action]
        self.q_table[state_id][action] = old_q + self.lr * (reward - old_q)

    def decay_epsilon(self):
        self.epsilon = max(0.01, self.epsilon * 0.995)

    def get_policy_entropy(self) -> float:
        total_entropy = 0
        for sid in self.q_table:
            qs = np.array(list(self.q_table[sid].values()))
            exp_qs = np.exp(qs) # Softmax logic simplified
            probs = exp_qs / np.sum(exp_qs)
            entropy = -np.sum(probs * np.log(probs + 1e-9))
            total_entropy += entropy
        return total_entropy / len(self.q_table)

# ==================== ë³´ìƒ ê³„ì‚° ì—”ì§„ ====================
def calculate_reward_vector(scn: Scenario, choice: str, weights: Dict[str, float]) -> float:
    """
    í•µì‹¬ ë¡œì§:
    ì‚¬ìš©ìžê°€ ì„¤ì •í•œ 'ê°€ì¹˜ê´€ ê°€ì¤‘ì¹˜(Weights)'ì™€ 
    ì‚¬ìš©ìžê°€ ì„¤ì •í•œ 'ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒ ë²¡í„°(Rewards)'ì˜ ë‚´ì (Dot Product)
    """
    r_vector = scn.rewards[choice]
    
    # ë‚´ì  ê³„ì‚° (Weights Â· Rewards)
    base_reward = sum(r_vector.get(fw, 0) * weights.get(fw, 0) for fw in FRAMEWORKS) * 10
    
    # êµ¬ì¡°ì  íŽ˜ë„í‹° (Optional: ìƒëª…/ê·œì¹™ ë“±)
    meta = scn.base[choice]
    lives_score = (meta["lives_saved"] - meta["lives_harmed"]) * 2
    
    return base_reward + lives_score

def calculate_diversity(choices_history: List[str]) -> float:
    if not choices_history: return 0.0
    a_count = choices_history.count("A")
    ratio = a_count / len(choices_history)
    return 1.0 - (2 * abs(0.5 - ratio))

# ==================== ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ í•¨ìˆ˜ ====================
def run_simulation(episodes: int, weights: Dict[str, float], custom_scenarios: List[Scenario]):
    agent = QLearningAgent(custom_scenarios, epsilon=0.5)
    history = []
    progress_bar = st.progress(0)
    
    for ep in range(episodes):
        ep_data = {"episode": ep + 1, "total_reward": 0, "actions": [], "consistency_sum": 0}
        
        for scn in custom_scenarios:
            action = agent.get_action(scn.sid)
            ep_data["actions"].append(action)
            
            # ì»¤ìŠ¤í…€ëœ ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒìœ¼ë¡œ ê³„ì‚°
            reward = calculate_reward_vector(scn, action, weights)
            ep_data["total_reward"] += reward
            
            agent.update(scn.sid, action, reward)
            
            # ì¼ê´€ì„± (ê°€ìž¥ ë†’ì€ ê°€ì¤‘ì¹˜ í”„ë ˆìž„ì›Œí¬ì™€ ì„ íƒì˜ ì •í•©ì„±)
            top_fw = max(weights, key=weights.get)
            # í•´ë‹¹ ì„ íƒì§€ê°€ top_fwì—ì„œ ì–‘ìˆ˜ ë³´ìƒì„ ì£¼ëŠ”ì§€ í™•ì¸
            is_consistent = 1.0 if scn.rewards[action][top_fw] > 0 else 0.0
            ep_data["consistency_sum"] += is_consistent

        agent.decay_epsilon()
        
        avg_consistency = ep_data["consistency_sum"] / len(custom_scenarios)
        entropy = agent.get_policy_entropy()
        diversity = calculate_diversity(ep_data["actions"])
        
        history.append({
            "Episode": ep + 1,
            "Total Reward": ep_data["total_reward"],
            "Strategy Entropy": entropy,
            "Diversity": diversity,
            "Ethical Consistency": avg_consistency
        })
        
        if (ep + 1) % 10 == 0:
            progress_bar.progress((ep + 1) / episodes)
            
    progress_bar.empty()
    return pd.DataFrame(history), agent

# ==================== UI êµ¬ì„± ====================
st.title("ðŸ§© ì‹œë‚˜ë¦¬ì˜¤ë³„ ë³´ìƒ ë²¡í„° ì»¤ìŠ¤í…€ ì‹œë®¬ë ˆì´ì…˜")
st.markdown("ë¬¸í™”ê¶Œì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì„¤ì •ê³¼ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë³´ìƒ êµ¬ì¡°ë¥¼ ìž…ë ¥í•˜ì—¬ AIì˜ ìœ¤ë¦¬ì  í•™ìŠµ ì–‘ìƒì„ ë¶„ì„í•©ë‹ˆë‹¤.")

# --- ì‚¬ì´ë“œë°”: ë¬¸í™”ê¶Œ ë° ê°€ì¹˜ê´€ ì„¤ì • ---
st.sidebar.header("1ï¸âƒ£ ë¬¸í™”ê¶Œ ë° ê°€ì¹˜ê´€ (Agent Context)")
culture_context = st.sidebar.text_input("ðŸŒ ë¬¸í™”ê¶Œ ìž…ë ¥", value="í˜„ëŒ€ í•œêµ­ ì‚¬íšŒ (ì¼ë°˜)", help="ë¶„ì„ ê²°ê³¼ì˜ ë¼ë²¨ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.")

st.sidebar.subheader("ê°€ì¹˜ê´€ ê°€ì¤‘ì¹˜ (Weights)")
st.sidebar.caption("AI ì—ì´ì „íŠ¸ê°€ ì–´ë–¤ ê°€ì¹˜ë¥¼ ì¤‘ìš”í•˜ê²Œ ì—¬ê¸°ëŠ”ì§€ ì„¤ì •")
w_emotion = st.sidebar.slider("ê°ì • (Emotion)", 0.0, 1.0, 0.5)
w_social = st.sidebar.slider("ì‚¬íšŒ (Social)", 0.0, 1.0, 0.2)
w_moral = st.sidebar.slider("ë„ë• (Moral)", 0.0, 1.0, 0.2)
w_identity = st.sidebar.slider("ì •ì²´ì„± (Identity)", 0.0, 1.0, 0.1)

# ê°€ì¤‘ì¹˜ ì •ê·œí™”
total_w = w_emotion + w_social + w_moral + w_identity
if total_w == 0: weights = {k: 0.25 for k in FRAMEWORKS}
else: weights = {"emotion": w_emotion/total_w, "social": w_social/total_w, "moral": w_moral/total_w, "identity": w_identity/total_w}

st.sidebar.divider()
st.sidebar.write(f"ðŸ· **ì„¤ì •ëœ ë¬¸í™”ê¶Œ:** {culture_context}")
st.sidebar.json(weights)

# --- ë©”ì¸ ì˜ì—­: ì‹œë‚˜ë¦¬ì˜¤ë³„ ë³´ìƒ ë²¡í„° ì„¤ì • ---
st.header("2ï¸âƒ£ ì‹œë‚˜ë¦¬ì˜¤ë³„ ë³´ìƒ ë²¡í„° ì„¤ì • (Environment Setup)")
st.caption("ê° ì‹œë‚˜ë¦¬ì˜¤ì˜ ì„ íƒì§€(A/B)ê°€ ì£¼ëŠ” ë³´ìƒ ê°’ì„ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤. (-1.0: ë§¤ìš° ë¶€ì •ì , +1.0: ë§¤ìš° ê¸ì •ì )")

custom_scenarios = []

# ì‹œë‚˜ë¦¬ì˜¤ ìž…ë ¥ ë£¨í”„
for idx, default_scn in enumerate(DEFAULT_SCENARIOS):
    with st.expander(f"ðŸ“ {default_scn.title} ì„¤ì • íŽ¼ì¹˜ê¸°", expanded=(idx==0)):
        st.write(f"**ìƒí™©**: {default_scn.setup}")
        
        col_a, col_b = st.columns(2)
        
        # ì„ íƒì§€ A ë³´ìƒ ìž…ë ¥
        with col_a:
            st.markdown(f"**ðŸ…° ì„ íƒì§€ A: {default_scn.options['A']}**")
            r_a = {}
            for fw in FRAMEWORKS:
                default_val = default_scn.rewards["A"].get(fw, 0.0)
                r_a[fw] = st.slider(f"[A] {fw} ë³´ìƒ", -1.0, 1.0, default_val, 0.1, key=f"s{idx}_a_{fw}")
        
        # ì„ íƒì§€ B ë³´ìƒ ìž…ë ¥
        with col_b:
            st.markdown(f"**ðŸ…± ì„ íƒì§€ B: {default_scn.options['B']}**")
            r_b = {}
            for fw in FRAMEWORKS:
                default_val = default_scn.rewards["B"].get(fw, 0.0)
                r_b[fw] = st.slider(f"[B] {fw} ë³´ìƒ", -1.0, 1.0, default_val, 0.1, key=f"s{idx}_b_{fw}")
        
        # ìˆ˜ì •ëœ ì‹œë‚˜ë¦¬ì˜¤ ê°ì²´ ìƒì„±
        new_scn = Scenario(
            sid=default_scn.sid,
            title=default_scn.title,
            setup=default_scn.setup,
            options=default_scn.options,
            votes=default_scn.votes,
            base=default_scn.base,
            rewards={"A": r_a, "B": r_b}  # ì‚¬ìš©ìžê°€ ìž…ë ¥í•œ ë³´ìƒ ë²¡í„° ì ìš©
        )
        custom_scenarios.append(new_scn)

# --- ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰ ---
st.divider()
st.header("3ï¸âƒ£ ì‹œë®¬ë ˆì´ì…˜ ë° ê²°ê³¼ ë¶„ì„")

col_run1, col_run2 = st.columns([1, 3])
with col_run1:
    episodes = st.number_input("í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜", 10, 2000, 200, step=50)
    btn_start = st.button("ðŸš€ ì‹œë®¬ë ˆì´ì…˜ ì‹œìž‘", type="primary")

if btn_start:
    with st.spinner(f"'{culture_context}' ë¬¸í™”ê¶Œ ì„¤ì •ìœ¼ë¡œ í•™ìŠµ ì¤‘..."):
        df_res, trained_agent = run_simulation(episodes, weights, custom_scenarios)
    
    st.success("ë¶„ì„ ì™„ë£Œ!")
    
    # ê²°ê³¼ ì‹œê°í™”
    tab1, tab2 = st.tabs(["ðŸ“Š í•™ìŠµ ì§€í‘œ (Metrics)", "ðŸ§  ìµœì¢… í•™ìŠµ ìƒíƒœ (Q-Table)"])
    
    with tab1:
        st.subheader(f"ðŸ“ˆ í•™ìŠµ ê³¡ì„  ({culture_context})")
        
        # 1. ë³´ìƒ ë° ì¼ê´€ì„±
        st.line_chart(
            df_res, x="Episode", y=["Total Reward", "Ethical Consistency"],
            color=["#FF5733", "#33FF57"]
        )
        
        col_m1, col_m2 = st.columns(2)
        # 2. ì—”íŠ¸ë¡œí”¼
        with col_m1:
            st.write("**ì „ëžµ ì—”íŠ¸ë¡œí”¼ (Entropy)** - íŒë‹¨ì˜ ë¶ˆí™•ì‹¤ì„±")
            st.line_chart(df_res, x="Episode", y="Strategy Entropy", color="#3357FF", height=200)
            
        # 3. ë‹¤ì–‘ì„±
        with col_m2:
            st.write("**í–‰ë™ ë‹¤ì–‘ì„± (Diversity)** - ì„ íƒì˜ ìœ ì—°ì„±")
            st.line_chart(df_res, x="Episode", y="Diversity", color="#FF33A1", height=200)

    with tab2:
        st.subheader("ðŸŽ¯ ì‹œë‚˜ë¦¬ì˜¤ë³„ ìµœì¢… ì„ í˜¸ë„")
        q_data = []
        for s in custom_scenarios:
            q = trained_agent.q_table[s.sid]
            choice = "A" if q["A"] > q["B"] else "B"
            q_data.append({
                "Scenario": s.title,
                "Option A (Score)": f"{q['A']:.2f}",
                "Option B (Score)": f"{q['B']:.2f}",
                "Final Choice": choice
            })
        st.table(pd.DataFrame(q_data))
