# app.py â€” Ethical Crossroads (DNA 2.0 ready) with RL & Analytics
# ì‘ì„±ì: Prof. Songhee Kang
# Optimized for: Automated RL Simulation & Strategy Analysis

import os, json, math, csv, io, datetime as dt, re
import random
import numpy as np
import pandas as pd
import streamlit as st
import httpx
from dataclasses import dataclass
from typing import Dict, Any, List, Tuple, Optional
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type

# ==================== ì„¤ì • ====================
st.set_page_config(page_title="Ethical Crossroads RL", page_icon="ğŸ§­", layout="wide")

# ==================== ë°ì´í„° ëª¨ë¸ ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    votes: Dict[str, str]
    base: Dict[str, Dict[str, float]]
    accept: Dict[str, float]
    rewards: Dict[str, Dict[str, float]]

# 5ê°œ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜ (ë³´ìƒ ë²¡í„° í¬í•¨)
SCENARIOS: List[Scenario] = [
    Scenario(
        sid="S1", title="1ë‹¨ê³„: ê³ ì „ì  íŠ¸ë¡¤ë¦¬",
        setup="ì„ ë¡œ ìœ„ 5ëª… vs 1ëª…. ë ˆë²„ë¥¼ ë‹¹ê¸¸ ê²ƒì¸ê°€?",
        options={"A": "1ëª… í¬ìƒ, 5ëª… êµ¬ì¡° (ê°œì…)", "B": "ë°©ê´€ (í˜„ìƒ ìœ ì§€)"},
        votes={"emotion":"A","social":"B","moral":"B","identity":"A"},
        base={"A": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.35, "rule_violation":0.60, "regret_risk":0.40},
              "B": {"lives_saved":0, "lives_harmed":5, "fairness_gap":0.50, "rule_violation":0.20, "regret_risk":0.60}},
        accept={"A":0.70, "B":0.50},
        rewards={"A": {"emotion": 1.0, "social": -0.5, "moral": -1.0, "identity": 0.5},
                 "B": {"emotion": -1.0, "social": 0.5, "moral": 1.0, "identity": -0.5}}
    ),
    Scenario(
        sid="S2", title="2ë‹¨ê³„: ë§¥ë½ì  ìš”ì†Œ",
        setup="ë¬´ë‹¨ ì¹¨ì…ì 5ëª… vs ê´€ë¦¬ì ìë…€ 1ëª….",
        options={"A": "5ëª… êµ¬ì¡° (ìë…€ í¬ìƒ)", "B": "ê·œì • ì¤€ìˆ˜ (5ëª… ë°©ê´€)"},
        votes={"emotion":"A","social":"B","moral":"B","identity":"B"},
        base={"A": {"lives_saved":5, "lives_harmed":1, "fairness_gap":0.65, "rule_violation":0.60, "regret_risk":0.70},
              "B": {"lives_saved":0, "lives_harmed":5, "fairness_gap":0.45, "rule_violation":0.25, "regret_risk":0.50}},
        accept={"A":0.35, "B":0.60},
        rewards={"A": {"emotion": 0.6, "social": -0.8, "moral": -0.7, "identity": 0.3},
                 "B": {"emotion": -0.5, "social": 0.9, "moral": 0.6, "identity": 0.4}}
    ),
    Scenario(
        sid="S3", title="3ë‹¨ê³„: ì˜ë£Œ ì¬ë‚œ ë¶„ë¥˜",
        setup="ì¼ë°˜ ë¶€ìƒì vs ìˆ™ë ¨ëœ ì˜ì‚¬(ì ì¬ ê°€ì¹˜).",
        options={"A": "ì˜ì‚¬ ìš°ì„  (ê³µë¦¬ì£¼ì˜)", "B": "ë™ë“± ëŒ€ìš° (í‰ë“±ì£¼ì˜)"},
        votes={"emotion":"A","social":"B","moral":"B","identity":"A"},
        base={"A": {"lives_saved":7, "lives_harmed":3, "fairness_gap":0.45, "rule_violation":0.35, "regret_risk":0.45},
              "B": {"lives_saved":6, "lives_harmed":4, "fairness_gap":0.30, "rule_violation":0.10, "regret_risk":0.35}},
        accept={"A":0.55, "B":0.65},
        rewards={"A": {"emotion": 0.7, "social": -0.4, "moral": -0.6, "identity": 0.8},
                 "B": {"emotion": -0.3, "social": 0.7, "moral": 0.9, "identity": 0.5}}
    ),
    Scenario(
        sid="S4", title="4ë‹¨ê³„: ììœ¨ì£¼í–‰ ë”œë ˆë§ˆ",
        setup="íƒ‘ìŠ¹ì(ê°œë°œì) 1ëª… vs ë³´í–‰ì 3ëª….",
        options={"A": "ë³´í–‰ì ë³´í˜¸ (íƒ‘ìŠ¹ì í¬ìƒ)", "B": "íƒ‘ìŠ¹ì ë³´í˜¸ (ë³´í–‰ì í¬ìƒ)"},
        votes={"emotion":"A","social":"B","moral":"A","identity":"A"},
        base={"A": {"lives_saved":3, "lives_harmed":1, "fairness_gap":0.35, "rule_violation":0.50, "regret_risk":0.55},
              "B": {"lives_saved":1, "lives_harmed":3, "fairness_gap":0.70, "rule_violation":0.60, "regret_risk":0.65}},
        accept={"A":0.60, "B":0.30},
        rewards={"A": {"emotion": 0.8, "social": -0.7, "moral": 0.6, "identity": -0.5},
                 "B": {"emotion": -0.9, "social": 0.8, "moral": -0.7, "identity": 0.9}}
    ),
    Scenario(
        sid="S5", title="5ë‹¨ê³„: ê·œì œ vs ììœ¨",
        setup="ì•ˆì „ ê·œì œ ê°•í™” vs ììœ¨ì„± ë³´ì¥.",
        options={"A": "ê·œì œ ê°•í™” (ì•ˆì „)", "B": "ììœ¨ì„± ë³´ì¥ (í˜ì‹ )"},
        votes={"emotion":"B","social":"A","moral":"A","identity":"B"},
        base={"A": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0.20, "rule_violation":0.10, "regret_risk":0.30},
              "B": {"lives_saved":0, "lives_harmed":0, "fairness_gap":0.40, "rule_violation":0.40, "regret_risk":0.40}},
        accept={"A":0.55, "B":0.55},
        rewards={"A": {"emotion": -0.3, "social": 0.9, "moral": 0.8, "identity": -0.6},
                 "B": {"emotion": 0.7, "social": -0.4, "moral": -0.5, "identity": 0.9}}
    )
]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

# ==================== ê°•í™”í•™ìŠµ (RL) ì—ì´ì „íŠ¸ ====================
class QLearningAgent:
    def __init__(self, learning_rate=0.1, discount_factor=0.9, epsilon=0.1):
        self.q_table = {s.sid: {"A": 0.0, "B": 0.0} for s in SCENARIOS}
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.episode_count = 0
        
    def get_action(self, state_id: str, explore: bool = True) -> str:
        # Epsilon-Greedy
        if explore and random.random() < self.epsilon:
            return random.choice(["A", "B"])
        
        qs = self.q_table[state_id]
        if qs["A"] > qs["B"]: return "A"
        elif qs["B"] > qs["A"]: return "B"
        return random.choice(["A", "B"])

    def update(self, state_id: str, action: str, reward: float):
        # Q(s,a) <- Q(s,a) + alpha * (reward - Q(s,a)) 
        # (ë‹¨ì¼ ìŠ¤í…ì´ë¯€ë¡œ gamma=0 í˜¹ì€ ë‹¤ìŒ ìƒíƒœ maxQ ìƒëµ ê°€ëŠ¥í•˜ë‚˜ ì¼ë°˜ì„±ì„ ìœ„í•´ ìœ ì§€)
        old_q = self.q_table[state_id][action]
        self.q_table[state_id][action] = old_q + self.lr * (reward - old_q)

    def decay_epsilon(self):
        self.epsilon = max(0.01, self.epsilon * 0.995)

    def get_policy_entropy(self) -> float:
        """í˜„ì¬ Qê°’ ê¸°ì¤€ Softmax í™•ë¥  ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ì „ëµì˜ ë¶ˆí™•ì‹¤ì„±)"""
        total_entropy = 0
        temperature = 1.0
        for sid in self.q_table:
            qs = np.array(list(self.q_table[sid].values()))
            # Softmax
            exp_qs = np.exp(qs / temperature)
            probs = exp_qs / np.sum(exp_qs)
            # Entropy = -sum(p * log(p))
            entropy = -np.sum(probs * np.log(probs + 1e-9))
            total_entropy += entropy
        return total_entropy / len(SCENARIOS)

# ==================== ë¶„ì„ ë° ì‹œë®¬ë ˆì´ì…˜ ì—”ì§„ ====================
def calculate_reward_vector(scn: Scenario, choice: str, weights: Dict[str, float]) -> float:
    """í”„ë ˆì„ì›Œí¬ ê°€ì¤‘ì¹˜ì™€ ì‹œë‚˜ë¦¬ì˜¤ ë³´ìƒì„ ë‚´ì (Dot Product)í•˜ì—¬ ë³´ìƒ ê³„ì‚°"""
    r_vector = scn.rewards[choice]
    # ê°€ì¤‘í•© (Reward)
    base_reward = sum(r_vector.get(fw, 0) * weights.get(fw, 0) for fw in FRAMEWORKS) * 10
    
    # ì¶”ê°€ í˜ë„í‹°/ë³´ë„ˆìŠ¤ (êµ¬ì¡°ì  ìš”ì†Œ)
    meta = scn.base[choice]
    lives_score = (meta["lives_saved"] - meta["lives_harmed"]) * 2
    penalty = (meta["rule_violation"] + meta["fairness_gap"]) * 2
    
    return base_reward + lives_score - penalty

def calculate_diversity(choices_history: List[str]) -> float:
    """ì„ íƒì˜ ë‹¤ì–‘ì„± (A/B ë¹„ìœ¨ì˜ ë¶„ì‚° ì—­ìˆ˜ ê°œë…)"""
    if not choices_history: return 0.0
    a_count = choices_history.count("A")
    ratio = a_count / len(choices_history)
    # 0.5ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘í•¨. 0ì´ë‚˜ 1ì´ë©´ íšì¼ì .
    # ì •ê·œí™”: 0.5ì¼ ë•Œ 1.0, 0 or 1ì¼ ë•Œ 0.0
    return 1.0 - (2 * abs(0.5 - ratio))

def run_simulation(episodes: int, weights: Dict[str, float]):
    agent = QLearningAgent(epsilon=0.5) # ì´ˆê¸° íƒí—˜ ë†’ê²Œ
    history = []
    
    progress_bar = st.progress(0)
    
    for ep in range(episodes):
        ep_data = {"episode": ep + 1, "total_reward": 0, "actions": [], "consistency_sum": 0}
        
        for scn in SCENARIOS:
            # 1. í–‰ë™ ì„ íƒ
            action = agent.get_action(scn.sid)
            ep_data["actions"].append(action)
            
            # 2. ë³´ìƒ ê³„ì‚°
            reward = calculate_reward_vector(scn, action, weights)
            ep_data["total_reward"] += reward
            
            # 3. í•™ìŠµ
            agent.update(scn.sid, action, reward)
            
            # 4. ì¼ê´€ì„± ì§€í‘œ (ì„ íƒì´ ê°€ì¤‘ì¹˜ ê°€ì¥ ë†’ì€ í”„ë ˆì„ì›Œí¬ì™€ ì¼ì¹˜í•˜ëŠ”ì§€)
            top_fw = max(weights, key=weights.get)
            match = 1.0 if scn.votes[top_fw] == action else 0.0
            ep_data["consistency_sum"] += match

        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ì²˜ë¦¬
        agent.decay_epsilon()
        
        # ì§€í‘œ ì €ì¥
        avg_consistency = ep_data["consistency_sum"] / len(SCENARIOS)
        entropy = agent.get_policy_entropy()
        diversity = calculate_diversity(ep_data["actions"])
        
        history.append({
            "Episode": ep + 1,
            "Total Reward": ep_data["total_reward"],
            "Strategy Entropy": entropy,
            "Diversity": diversity,
            "Ethical Consistency": avg_consistency,
            "Epsilon": agent.epsilon
        })
        
        if (ep + 1) % 10 == 0:
            progress_bar.progress((ep + 1) / episodes)
            
    progress_bar.empty()
    return pd.DataFrame(history), agent

# ==================== UI êµ¬ì„± ====================
st.title("ğŸ¤– ìœ¤ë¦¬ì  ê°•í™”í•™ìŠµ ì‹œë®¬ë ˆì´í„° (RL Analytics)")
st.markdown("""
ì´ ì‹œìŠ¤í…œì€ AIê°€ **ì£¼ì–´ì§„ ìœ¤ë¦¬ ê°€ì¤‘ì¹˜(ë³´ìƒ ë²¡í„°)**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ê³¼ì •ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
- **Entropy**: ì „ëµì˜ ë¶ˆí™•ì‹¤ì„± (ë‚®ì„ìˆ˜ë¡ í™•ê³ í•œ ì‹ ë… í˜•ì„±)
- **Diversity**: ì„ íƒì˜ ë‹¤ì–‘ì„± (ìƒí™©ì— ë”°ë¥¸ ìœ ì—°í•œ ëŒ€ì²˜)
- **Consistency**: ì„¤ì •ëœ ìœ¤ë¦¬ê´€ê³¼ì˜ ì¼ì¹˜ë„
""")

# --- ì‚¬ì´ë“œë°”: ê°€ì¤‘ì¹˜ ì„¤ì • ---
st.sidebar.header("âš–ï¸ ê°€ì¤‘ì¹˜ ì„¤ì • (Reward Weights)")
w_emotion = st.sidebar.slider("ê°ì • (Emotion)", 0.0, 1.0, 0.5)
w_social = st.sidebar.slider("ì‚¬íšŒ (Social)", 0.0, 1.0, 0.2)
w_moral = st.sidebar.slider("ë„ë• (Moral)", 0.0, 1.0, 0.2)
w_identity = st.sidebar.slider("ì •ì²´ì„± (Identity)", 0.0, 1.0, 0.1)

# ì •ê·œí™”
total_w = w_emotion + w_social + w_moral + w_identity
if total_w == 0: weights = {k: 0.25 for k in FRAMEWORKS}
else: weights = {"emotion": w_emotion/total_w, "social": w_social/total_w, "moral": w_moral/total_w, "identity": w_identity/total_w}

st.sidebar.markdown("---")
st.sidebar.write("ğŸ“Š **ì…ë ¥ëœ ê°€ì¤‘ì¹˜ ë¹„ìœ¨**")
st.sidebar.json(weights)

# --- ë©”ì¸ íƒ­ ---
tab1, tab2 = st.tabs(["ğŸš€ ìë™ ì‹œë®¬ë ˆì´ì…˜", "ğŸ® ìˆ˜ë™ í”Œë ˆì´"])

with tab1:
    st.subheader("ê³ ì† í•™ìŠµ ì‹œë®¬ë ˆì´ì…˜")
    col1, col2 = st.columns([1, 3])
    
    with col1:
        n_episodes = st.number_input("ì—í”¼ì†Œë“œ ìˆ˜", min_value=10, max_value=2000, value=200, step=10)
        start_sim = st.button("ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘", type="primary")

    if start_sim:
        with st.spinner("AIê°€ ìœ¤ë¦¬ì  ë”œë ˆë§ˆë¥¼ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤..."):
            df_res, trained_agent = run_simulation(n_episodes, weights)
        
        st.success("í•™ìŠµ ì™„ë£Œ!")
        
        # 1. ë³´ìƒ ë° ì¼ê´€ì„± ê·¸ë˜í”„
        st.subheader("ğŸ“ˆ í•™ìŠµ ê³¡ì„ ")
        chart_data = df_res[["Episode", "Total Reward", "Ethical Consistency"]].melt('Episode')
        st.line_chart(
            df_res, x="Episode", y=["Total Reward", "Ethical Consistency"],
            color=["#FF5733", "#33FF57"], height=300
        )
        
        # 2. ê³ ê¸‰ ì§€í‘œ (ì—”íŠ¸ë¡œí”¼ & ë‹¤ì–‘ì„±)
        st.subheader("ğŸ§  ì „ëµ ë¶„ì„ ì§€í‘œ")
        col_m1, col_m2 = st.columns(2)
        
        with col_m1:
            st.markdown("**ì „ëµ ì—”íŠ¸ë¡œí”¼ (Strategy Entropy)**")
            st.caption("ê°’ì´ ë‚®ì•„ì§ˆìˆ˜ë¡ AIê°€ í™•ê³ í•œ ìœ¤ë¦¬ì  íŒë‹¨ ê¸°ì¤€ì„ ì„¸ì› ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
            st.line_chart(df_res, x="Episode", y="Strategy Entropy", color="#3357FF", height=250)
            
        with col_m2:
            st.markdown("**í–‰ë™ ë‹¤ì–‘ì„± (Action Diversity)**")
            st.caption("1.0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ A/Bë¥¼ ìƒí™©ì— ë§ê²Œ ì„ì–´ì„œ ì„ íƒí•©ë‹ˆë‹¤.")
            st.line_chart(df_res, x="Episode", y="Diversity", color="#FF33A1", height=250)
            
        # 3. ìµœì¢… Q-Table íˆíŠ¸ë§µ ìœ ì‚¬ ì‹œê°í™”
        st.subheader("ğŸ¯ ìµœì¢… í•™ìŠµ ê²°ê³¼ (Q-Values)")
        q_data = []
        for sid, q in trained_agent.q_table.items():
            best = "A" if q["A"] > q["B"] else "B"
            q_data.append({
                "Scenario": sid, 
                "Score A": round(q["A"], 2), 
                "Score B": round(q["B"], 2),
                "Choice": best
            })
        st.dataframe(pd.DataFrame(q_data).set_index("Scenario"), use_container_width=True)

with tab2:
    st.info("ê¸°ì¡´ì˜ ìˆ˜ë™ í”Œë ˆì´ ëª¨ë“œì…ë‹ˆë‹¤. (í•™ìŠµëœ ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ìš©)")
    if 'rl_agent' not in st.session_state:
        st.session_state.rl_agent = QLearningAgent()
        
    # ê°„ë‹¨í•œ í”Œë ˆì´ UI (ê¸°ì¡´ ì½”ë“œì˜ ì¶•ì•½íŒ)
    current_scn_idx = st.session_state.get('scn_idx', 0)
    
    if current_scn_idx < len(SCENARIOS):
        s = SCENARIOS[current_scn_idx]
        st.markdown(f"**{s.title}**")
        st.write(s.setup)
        
        c1, c2 = st.columns(2)
        if c1.button("A ì„ íƒ"):
            r = calculate_reward_vector(s, "A", weights)
            st.session_state.rl_agent.update(s.sid, "A", r)
            st.toast(f"ë³´ìƒ: {r:.1f}")
            st.session_state.scn_idx = current_scn_idx + 1
            st.rerun()
            
        if c2.button("B ì„ íƒ"):
            r = calculate_reward_vector(s, "B", weights)
            st.session_state.rl_agent.update(s.sid, "B", r)
            st.toast(f"ë³´ìƒ: {r:.1f}")
            st.session_state.scn_idx = current_scn_idx + 1
            st.rerun()
    else:
        st.success("ëª¨ë“  ë¼ìš´ë“œ ì¢…ë£Œ")
        if st.button("ë‹¤ì‹œ í•˜ê¸°"):
            st.session_state.scn_idx = 0
            st.rerun()
