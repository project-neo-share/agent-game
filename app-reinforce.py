# app.py â€” Cultural AI Ethics: Intra-Cultural Correlation Analysis
# ì‘ì„±ì: Prof. Songhee Kang
# Update: Single Culture Focus, Diversity-Reward Correlation

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
from scipy.stats import pearsonr
from dataclasses import dataclass
from typing import Dict, List

# ==================== ì„¤ì • ====================
st.set_page_config(page_title="AI Ethics: Culture & Correlation", page_icon="ğŸ”¬", layout="wide")

# ==================== ë°ì´í„° ëª¨ë¸ ====================
@dataclass
class Scenario:
    sid: str
    title: str
    setup: str
    options: Dict[str, str]
    rewards: Dict[str, Dict[str, float]]

# ê¸°ë³¸ ì‹œë‚˜ë¦¬ì˜¤
SCENARIOS = [
    Scenario(
        sid="S1", title="1ë‹¨ê³„: ê³ ì „ì  íŠ¸ë¡¤ë¦¬",
        setup="ì„ ë¡œ ìœ„ 5ëª… vs 1ëª…. ë ˆë²„ë¥¼ ë‹¹ê¸¸ ê²ƒì¸ê°€?",
        options={"A": "1ëª… í¬ìƒ (ê°œì…)", "B": "ë°©ê´€ (í˜„ìƒ ìœ ì§€)"},
        rewards={"A": {"emotion": 1.0, "social": -0.5, "moral": -1.0, "identity": 0.5},
                 "B": {"emotion": -1.0, "social": 0.5, "moral": 1.0, "identity": -0.5}}
    ),
    Scenario(
        sid="S2", title="2ë‹¨ê³„: ë§¥ë½ì  ìš”ì†Œ",
        setup="ë¬´ë‹¨ ì¹¨ì…ì 5ëª… vs ê´€ë¦¬ì ìë…€ 1ëª….",
        options={"A": "5ëª… êµ¬ì¡° (ìë…€ í¬ìƒ)", "B": "ê·œì • ì¤€ìˆ˜ (5ëª… ë°©ê´€)"},
        rewards={"A": {"emotion": 0.6, "social": -0.8, "moral": -0.7, "identity": 0.3},
                 "B": {"emotion": -0.5, "social": 0.9, "moral": 0.6, "identity": 0.4}}
    ),
    Scenario(
        sid="S3", title="3ë‹¨ê³„: ì˜ë£Œ ì¬ë‚œ ë¶„ë¥˜",
        setup="ì¼ë°˜ ë¶€ìƒì ë‹¤ìˆ˜ vs ìˆ™ë ¨ëœ ì˜ì‚¬ 1ëª….",
        options={"A": "ì˜ì‚¬ ìš°ì„  (ê³µë¦¬ì£¼ì˜)", "B": "ë™ë“± ëŒ€ìš° (í‰ë“±ì£¼ì˜)"},
        rewards={"A": {"emotion": 0.7, "social": -0.4, "moral": -0.6, "identity": 0.8},
                 "B": {"emotion": -0.3, "social": 0.7, "moral": 0.9, "identity": 0.5}}
    ),
]

FRAMEWORKS = ["emotion", "social", "moral", "identity"]

# ë¬¸í™”ê¶Œ í”„ë¦¬ì…‹
CULTURES_PRESETS = {
    "USA":      {"emotion": 0.3, "social": 0.1, "identity": 0.3, "moral": 0.3},
    "CHINA":    {"emotion": 0.1, "social": 0.5, "identity": 0.2, "moral": 0.2},
    "EUROPE":   {"emotion": 0.3, "social": 0.2, "identity": 0.2, "moral": 0.3},
    "KOREA":    {"emotion": 0.2, "social": 0.2, "identity": 0.4, "moral": 0.2},
    "LATIN_AM": {"emotion": 0.4, "social": 0.2, "identity": 0.2, "moral": 0.2},
    "MIDDLE_E": {"emotion": 0.1, "social": 0.2, "identity": 0.2, "moral": 0.5},
    "AFRICA":   {"emotion": 0.2, "social": 0.4, "identity": 0.2, "moral": 0.2},
}

# ==================== ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ====================
class QLearningAgent:
    def __init__(self, name, weights, scenarios, learning_rate=0.1, epsilon=0.5):
        self.name = name
        self.weights = weights
        self.scenarios = scenarios
        self.lr = learning_rate
        self.epsilon = epsilon
        self.q_table = {s.sid: {"A": 0.0, "B": 0.0} for s in scenarios}
        
    def get_action(self, sid):
        if random.random() < self.epsilon:
            return random.choice(["A", "B"])
        qs = self.q_table[sid]
        return "A" if qs["A"] > qs["B"] else "B"

    def calculate_reward(self, sid, action):
        scn = next(s for s in self.scenarios if s.sid == sid)
        r_vec = scn.rewards[action]
        reward = sum(r_vec.get(k, 0) * self.weights.get(k, 0) for k in FRAMEWORKS) * 10
        return reward

    def update(self, sid, action, reward):
        old_q = self.q_table[sid][action]
        self.q_table[sid][action] = old_q + self.lr * (reward - old_q)

    def decay_epsilon(self):
        self.epsilon = max(0.01, self.epsilon * 0.99)

# ==================== ë¶„ì„ í•¨ìˆ˜ ====================
def calculate_diversity(actions_list: List[str]) -> float:
    """í–‰ë™ ë‹¤ì–‘ì„± ê³„ì‚° (0.0: íšì¼ì  ~ 1.0: ì™„ì „ ê· í˜•)"""
    if not actions_list: return 0.0
    a_count = actions_list.count("A")
    ratio = a_count / len(actions_list)
    # 0.5(ë°˜ë°˜)ì¼ ë•Œ 1.0, 0 ë˜ëŠ” 1ì¼ ë•Œ 0.0ì´ ë˜ë„ë¡ ì •ê·œí™”
    return 1.0 - (2 * abs(0.5 - ratio))

def run_single_culture_simulation(culture_name, weights, episodes):
    agent = QLearningAgent(culture_name, weights, SCENARIOS)
    
    history = {
        "episode": [],
        "reward": [],
        "diversity": []
    }
    
    progress = st.progress(0)
    
    for ep in range(episodes):
        ep_actions = []
        ep_reward = 0
        
        for scn in SCENARIOS:
            action = agent.get_action(scn.sid)
            reward = agent.calculate_reward(scn.sid, action)
            agent.update(scn.sid, action, reward)
            
            ep_actions.append(action)
            ep_reward += reward
            
        agent.decay_epsilon()
        
        # ì§€í‘œ ê¸°ë¡
        history["episode"].append(ep + 1)
        history["reward"].append(ep_reward)
        # ì´ë²ˆ ì—í”¼ì†Œë“œì˜ í–‰ë™ ë‹¤ì–‘ì„± (S1, S2, S3ì˜ ì„ íƒì´ ì–¼ë§ˆë‚˜ ì„ì˜€ëŠ”ì§€)
        history["diversity"].append(calculate_diversity(ep_actions))
        
        if (ep + 1) % 10 == 0:
            progress.progress((ep + 1) / episodes)
            
    progress.empty()
    return pd.DataFrame(history)

# ==================== UI êµ¬ì„± ====================
st.title("ğŸ”¬ AI Ethics: ê°•í™”í•™ìŠµ ì—ì´ì „íŠ¸ ì‹œë®¬ë ˆì´ì…˜")
st.markdown("""
íŠ¹ì • ë¬¸í™”ê¶Œ ë‚´ì—ì„œ AIê°€ í•™ìŠµí•  ë•Œ 'í–‰ë™ì˜ ë‹¤ì–‘ì„±(Behavioral Diversity)'ê³¼ 'íšë“í•œ ë³´ìƒ(Reward)' ê°„ì— 
ì–´ë–¤ ìƒê´€ê´€ê³„ê°€ ìˆëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
""")

# --- ì‚¬ì´ë“œë°” ---
st.sidebar.header("âš™ï¸ ì„¤ì • (Settings)")

# 1. ë¬¸í™”ê¶Œ ì„ íƒ (ë‹¨ì¼ ì„ íƒ)
selected_culture = st.sidebar.selectbox(
    "ë¶„ì„í•  ë¬¸í™”ê¶Œ ì„ íƒ", 
    list(CULTURES_PRESETS.keys()),
    index=3 # Default to KOREA
)

# 2. íŒŒë¼ë¯¸í„°
episodes = st.sidebar.slider("í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜", 100, 1000, 300, step=50)

# ê°€ì¤‘ì¹˜ í™•ì¸ ë° ì»¤ìŠ¤í…€ (ë¬¸í™”ê¶Œ ë‚´ ë¯¸ì„¸ ì¡°ì •)
st.sidebar.markdown("---")
st.sidebar.subheader(f"{selected_culture} ê°€ì¤‘ì¹˜ ìƒì„¸")
current_weights = CULTURES_PRESETS[selected_culture].copy()
use_custom = st.sidebar.checkbox("ê°€ì¤‘ì¹˜ ë¯¸ì„¸ ì¡°ì •í•˜ê¸°", False)

if use_custom:
    for k in FRAMEWORKS:
        current_weights[k] = st.sidebar.slider(f"{k}", 0.0, 1.0, current_weights[k])
    # ì •ê·œí™”
    total_w = sum(current_weights.values()) or 1
    current_weights = {k: v/total_w for k, v in current_weights.items()}
else:
    st.sidebar.json(current_weights)

# --- ë©”ì¸ ì‹¤í–‰ ---
if st.button("ğŸš€ ë¶„ì„ ì‹œì‘ (Analyze)", type="primary"):
    with st.spinner(f"'{selected_culture}' ë¬¸í™”ê¶Œ ì‹œë®¬ë ˆì´ì…˜ ì¤‘..."):
        df = run_single_culture_simulation(selected_culture, current_weights, episodes)
    
    st.success("ë¶„ì„ ì™„ë£Œ!")
    
    # 1. ì‹œê³„ì—´ ê·¸ë˜í”„ (Reward & Diversity)
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“ˆ í•™ìŠµ ê³¡ì„  (Learning Curve)")
        st.caption("ì—í”¼ì†Œë“œê°€ ì§„í–‰ë¨ì— ë”°ë¼ AIê°€ íšë“í•œ ì´ ë³´ìƒì˜ ë³€í™”")
        st.line_chart(df, x="episode", y="reward", color="#FF4B4B")
        
    with col2:
        st.subheader("ğŸ”€ í–‰ë™ ë‹¤ì–‘ì„± (Behavioral Diversity)")
        st.caption("ì„ íƒì˜ ë‹¤ì–‘ì„± (0: í•œìª½ìœ¼ë¡œ ì ë¦¼, 1: A/B ê³¨ê³ ë£¨ ì„ íƒ)")
        st.line_chart(df, x="episode", y="diversity", color="#1F77B4")
        
    st.markdown("---")
    
    # 2. ìƒê´€ê´€ê³„ ë¶„ì„ (Scatter Plot)
    st.subheader("ğŸ”— ìƒê´€ê´€ê³„ ë¶„ì„: ë‹¤ì–‘ì„± vs ë³´ìƒ")
    
    # ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    r_val, p_val = pearsonr(df["diversity"], df["reward"])
    
    col_corr1, col_corr2 = st.columns([2, 1])
    
    with col_corr1:
        fig, ax = plt.subplots(figsize=(8, 5))
        ax.scatter(df["diversity"], df["reward"], alpha=0.5, c='purple')
        ax.set_xlabel("Behavioral Diversity")
        ax.set_ylabel("Total Reward")
        ax.set_title(f"Diversity vs Reward (Scatter)")
        ax.grid(True, alpha=0.3)
        st.pyplot(fig)
        
    with col_corr2:
        st.markdown("### ğŸ“Š í†µê³„ ìš”ì•½")
        st.metric("í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ (r)", f"{r_val:.3f}")
        st.metric("P-value", f"{p_val:.3e}")
        
        st.markdown("---")
        st.markdown("**ğŸ’¡ í•´ì„ ê°€ì´ë“œ**")
        if r_val > 0.3:
            st.info("ì–‘ì˜ ìƒê´€ê´€ê³„: ë‹¤ì–‘í•˜ê²Œ ì‹œë„í• ìˆ˜ë¡ ë” ë†’ì€ ë³´ìƒì„ ì–»ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.")
        elif r_val < -0.3:
            st.warning("ìŒì˜ ìƒê´€ê´€ê³„: íŠ¹ì • í–‰ë™ì— ì§‘ì¤‘(ë‹¤ì–‘ì„± ë‚®ìŒ)í•´ì•¼ ë³´ìƒì´ ë†’ì•„ì§‘ë‹ˆë‹¤.")
        else:
            st.write("ëšœë ·í•œ ìƒê´€ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ë³´ìƒì€ ë‹¤ì–‘ì„±ê³¼ ë¬´ê´€í•˜ê²Œ ê²°ì •ë©ë‹ˆë‹¤.")

    # 3. ë°ì´í„° ë‹¤ìš´ë¡œë“œ
    with st.expander("ğŸ“¥ ë¡œìš° ë°ì´í„°(Raw Data) ë³´ê¸°"):
        st.dataframe(df)
        st.download_button(
            "CSV ë‹¤ìš´ë¡œë“œ", 
            df.to_csv(index=False), 
            file_name=f"{selected_culture}_simulation_data.csv"
        )
