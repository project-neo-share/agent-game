# 🎓 업데이트 노트: 윤리 AI 에이전트 시뮬레이션 (v3.0)

안녕하세요, 학생 여러분! 👋
이번 업데이트에서는 **인공지능이 윤리적 결정을 어떻게 학습하는지** 눈으로 직접 확인할 수 있는 시뮬레이터를 구현했습니다.

특히 이번 버전은 **강화학습의 가장 기초적인 원리**(E-Greedy)를 사용하여, AI가 가치관(문화)에 따라 딜레마 상황에서 어떤 선택을 내리는지 관찰할 수 있습니다.

---

## 1. 핵심 알고리즘: E-Greedy (Epsilon-Greedy) 란? 🤖

우리가 구현한 AI 에이전트는 **'E-Greedy(입실론-그리디)'** 라는 전략을 사용합니다. 이름은 어렵지만, 원리는 우리가 점심 메뉴를 고르는 과정과 아주 비슷합니다!

### 🍔 맛집 탐방 비유
여러분이 학교 앞 식당에서 점심을 먹는다고 상상해보세요.

1.  **활용 (Exploitation) = "아는 맛이 최고야"**
    * 이미 먹어본 메뉴 중 **가장 맛있었던(점수가 높았던)** 메뉴를 다시 시킵니다.
    * 실패할 확률은 적지만, 더 맛있는 새로운 메뉴를 발견할 기회는 없습니다.
2.  **탐험 (Exploration) = "새로운 도전!"**
    * 맛이 있을지 없을지 모르지만, **무작위로** 아무 메뉴나 시켜봅니다.
    * 실패할 수도 있지만, 의외의 '인생 메뉴'를 찾을 수도 있습니다.

### 🎲 E-Greedy의 작동 원리
AI는 동전을 던져서 이 두 가지 행동 중 하나를 결정합니다.

* **Epsilon ($\epsilon$) 확률로 '탐험'**: "이번엔 10% 확률로 아무거나 눌러보자!" (새로운 데이터 수집)
* **나머지 확률로 '활용'**: "그동안 해본 것 중에 점수가 제일 높았던 걸 선택하자!" (최적의 선택)

> **💡 학습 포인트:**
> 시뮬레이션 초반에는 AI가 이것저것 무작위로 시도해보지만(탐험), 학습이 거듭될수록($\epsilon$ 감소) 점점 자신의 가치관에 맞는 선택(활용)을 고집하게 됩니다. 이를 **탐험과 활용의 트레이드오프**(Exploration-Exploitation Trade-off)라고 합니다.

---

## 2. 시뮬레이션 구성 요소 🌍

이 게임은 크게 두 가지 설정이 만나는 구조입니다.

### 🅰️ 환경 (Environment): "게임의 규칙"
* **시나리오 보상 벡터**: 각 시나리오(트롤리 딜레마 등)에서 A나 B를 선택했을 때, 사회가 어떤 평가를 내리는지 설정합니다.
* 예: "사람을 구하면(A) 감정 점수 +1.0, 규칙을 어기면 도덕 점수 -0.5"

### 👤 에이전트 (Agent): "플레이어의 성격"
* **문화권 가중치**: AI가 어떤 가치를 더 중요하게 여기는지 설정합니다.
* 예: "나는 감정(Emotion)보다 사회적 규칙(Social)이 더 중요해!" (가중치 조절)

👉 **결과:** AI는 **[환경이 주는 점수] × [내 가중치]** 가 가장 높은 선택지를 찾으려 노력합니다.

---

## 3. 주요 학습 지표 (그래프 해석법) 📈

시뮬레이션이 끝나면 3가지 그래프가 나옵니다. 각각이 무엇을 의미하는지 알아봅시다.

### 1️⃣ 총 보상 (Total Reward)
* **의미:** AI가 자신의 가치관에 맞는 선택을 얼마나 잘 했는가?
* **해석:** 그래프가 **우상향(↗)** 한다면, AI가 시행착오 끝에 자신에게 이득이 되는(칭찬받는) 선택지를 찾아냈다는 뜻입니다.

### 2️⃣ 전략 엔트로피 (Strategy Entropy) 📉
* **의미:** AI가 선택을 할 때 **얼마나 고민(망설임)하고 있는가?**
* **해석:**
    * **높음:** "A를 할까 B를 할까? 아직 잘 모르겠어..." (불확실함)
    * **낮음:** "난 무조건 A야!" (확신)
* **학습 과정:** 초반에는 높았다가, 학습이 진행될수록 **점점 낮아져야(↘)** 정상적인 학습입니다. AI가 확고한 신념을 갖게 되는 과정입니다.

### 3️⃣ 행동 다양성 (Behavioral Diversity) 🔀
* **의미:** AI가 A와 B를 골고루 선택하는가? 아니면 한쪽만 선택하는가?
* **해석:**
    * **1.0에 가까움:** A와 B를 비슷하게 선택함 (유연함 또는 아직 탐험 중)
    * **0.0에 가까움:** 한 가지 선택만 고집함 (편향됨)
* **상관관계 분석:** 다양성이 높을 때 보상이 높은지, 낮을 때 높은지를 통해 **"이 문화권은 융통성이 필요한가, 원칙 준수가 필요한가?"**를 분석할 수 있습니다.

---

## 4. 실습 가이드 🚀

1.  **환경 설정:** 시나리오별로 A/B 선택지가 어떤 보상(감정, 도덕 등)을 주는지 설정해보세요.
2.  **에이전트 설정:** '한국', '미국' 등 문화권을 선택하거나 직접 가중치를 수정해보세요.
3.  **시뮬레이션 시작:** AI가 수백 번의 시행착오를 겪으며 어떻게 변화하는지 지켜보세요.
4.  **결과 토의:**
    * "왜 엔트로피가 빨리 떨어졌을까?" (답이 뻔한 상황이었나?)
    * "왜 다양성이 낮은데 보상은 높을까?" (한 우물만 파는 게 유리했나?)

여러분의 AI 에이전트는 어떤 윤리관을 형성하게 될까요? 지금 바로 실험해보세요!
